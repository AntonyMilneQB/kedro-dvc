# Kedro-dvc Design

Use kedro and dvc together to track and distribute experiments.

## Overview

Kedro creates and maintains machine-learning pipelines. DVC tracks
changes to data together with changes to code in the git repo, and
allows experiment tracking via git commits.

We would like to use dvc experiment tracking to track workflows
generated by kedro. As Kedro, unlike dvc, is not language neutral,
we can trace what code is run during pipeline execution. When
rerunning an experiment we can use this information to invalidate
steps when code changes.

Also, kedro is able to to generate distributed pipelines using
templates, for instance, to run on kubernetes via argo. We want to
pull together artifacts run by distributed code into a dvc repo,
so that dvc experiment tracking can also be used with distributed
execution in kubernetes.

## Goals:

* Generate .dvc files to track artifacts from kedro inputs & outputs
* Generate dvc pipelines from kedro workflows
* Use dvc experiment tracking on kedro workflows
* Invalidate steps based on both data and code change
* Distribute dvc experiments to kubernetes

## Resources

* [kedro discussion with some links](https://github.com/kedro-org/kedro/discussions/837)
* [dvc discussion](https://discord.com/channels/485586884165107732/938821298929430548/939175277228072970)

* kedro folks most interested in using dvc in conjunction with trace
  to decide what parts of pipeline to rerun.
* dvc @dberenbaum: notes no api yet, but "...the internals are fairly easy to interact with if you want to give it a try using dvc.repo.Repo"

## Design & Roadmap

### Stage 1: setup correspondence between kedro and dvc

* Map kedro node inputs and outputs to dvc tracked artifacts

Kedro has a data catalog for initial data. Subsequent artifacts may be
passed in memory. At first, we will only support inter-operabiity
between inputs and outputs which are saved on disk. We will only also
only support "local" files. Dvc supports remote data; later we can
map (a subset of) other kedro storage types to dvc types.

Dvc keeps track of different types of artifact: data, parameters, metrics, plots. We should be able to distinguish between these using
the type of the items in the data catalog. However, we may require
additional annotations.

* Map pipelines into dvc.yaml files

Each dvc stage corresponds to running of one node. At first we will only support local runner.

* Map kedro versioned data onto dvc experiment stages

This requires corresponding commits to version. Running via dvc
would require many subprocess invocations, but should be possible. Running via kedro runner with hooks to maintain dvc state will
be desirable.

### Stage 2: Code-aware experiments

Use dvc experiment tracking to keep track of different versions of a project. Use kedro hooks with [trace](https://docs.python.org/3/library/trace.html) to optionally skip steps.

* [@mzip2 example skip hook](https://gist.github.com/mzjp2/076bfd73b0215bda01ee71186966389d) but without trace.

Pipelines will have to be run in a number of different modes:

* Run everything, snapshot code dependencies
* Run as required, using code dependencies stored and/or dvc versions
* Run without trace for better performance

The intention is to use dvc to manage experiment versions. This may
require additional coupling between kedro runner and dvc; however,
by default we can use [dvclive](https://github.com/iterative/dvclive)
where possible to encapsulate dvc.

### Stage 3: Distributed experiments (Kubernetes)

There are two ways to interact with dvc when distributing python steps: 
(1) There is one dvc repo; workers use hooks to fetch/store state; (2)
Each worker has a copy of the repo. Use git mechanisms to merge state.

In the first case, the central repo could be on a shared volume. However, to avoid corrupting state, we probably need to have only one node maintaining dvc state. Alternately, we could make the workers
completely oblivious to dvc, except that they call a checkpoint api
after running a node, which collects artifacts and maintains dvc state. In either case, a "runner node" would have to provide an API.

The second case does not require a central server, but might be slow if artifacts must go -- e.g. -- via s3 for every step. Perhaps this could be mitigated by using in-cluster minio as artifact storage.

